{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary = {}\n",
    "Begining = {}\n",
    "Middle = {}\n",
    "Ending = {}\n",
    "WordFreq = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 689\n"
     ]
    }
   ],
   "source": [
    "dict_file = open('dict.txt', 'r')\n",
    "dict_Lines = dict_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in dict_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    if len(splited) != 2: continue\n",
    "    key = splited[0]\n",
    "    value = splited[1]\n",
    "    Dictionary[key] = value\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "توکیو\n",
      "وی‌او‌ای\n",
      "سس\n",
      "زهرا\n"
     ]
    }
   ],
   "source": [
    "print(Dictionary[\"tokyo\"])\n",
    "print(Dictionary[\"voa\"])\n",
    "print(Dictionary[\"sos\"])\n",
    "print(Dictionary[\"zahra\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 38\n"
     ]
    }
   ],
   "source": [
    "beg_file = open('beginning.txt', 'r')\n",
    "beg_Lines = beg_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in beg_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Begining[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['خا', 'خوا', 'خ']\n",
      "['ش']\n",
      "['ج', 'ژ']\n",
      "['چ']\n"
     ]
    }
   ],
   "source": [
    "print(Begining[\"kha\"])\n",
    "print(Begining[\"sh\"])\n",
    "print(Begining[\"j\"])\n",
    "print(Begining[\"ch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 45\n"
     ]
    }
   ],
   "source": [
    "mid_file = open('middle.txt', 'r')\n",
    "mid_Lines = mid_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in mid_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Middle[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ش']\n",
      "['ی', 'یع', 'ئی', 'عی']\n",
      "['ک']\n",
      "['nothing', 'ع', 'ا', 'اع', 'أ', 'ؤ', 'ؤا', 'ئا', 'ئ']\n"
     ]
    }
   ],
   "source": [
    "print(Middle[\"sh\"])\n",
    "print(Middle[\"i\"])\n",
    "print(Middle[\"ck\"])\n",
    "print(Middle[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 47\n"
     ]
    }
   ],
   "source": [
    "end_file = open('ending.txt', 'r')\n",
    "end_Lines = end_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in end_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Ending[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ش']\n",
      "['ی', 'یع']\n",
      "['ک']\n",
      "['ا', 'اع', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "print(Ending[\"sh\"])\n",
    "print(Ending[\"i\"])\n",
    "print(Ending[\"ck\"])\n",
    "print(Ending[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.WordFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 111302\n"
     ]
    }
   ],
   "source": [
    "freq_file = open('persian-wikipedia.txt', 'r')\n",
    "freq_Lines = freq_file.readlines()\n",
    " \n",
    "min_freq = 4\n",
    "count = 0\n",
    "for line in freq_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\"\\t\")\n",
    "    word = splited[0]\n",
    "    freq = int(splited[1]) \n",
    "    if freq < min_freq:\n",
    "        break\n",
    "    if word not in WordFreq.keys():\n",
    "        WordFreq[word] = freq\n",
    "\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84193\n",
      "24228\n",
      "5\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "print(WordFreq[\"یک\"])\n",
    "print(WordFreq[\"جمعیت\"])\n",
    "print(WordFreq[\"گودترین\"])\n",
    "print(WordFreq[\"لطفا\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "sep_regex = re.compile(r'[ \\-_~!@#%$^&*\\(\\)\\[\\]\\{\\}/\\:;\"|,./?`]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_word_internal(word, original_word):\n",
    "    # this function receives the word as separate letters\n",
    "    persian = []\n",
    "    for i, letter in enumerate(word):\n",
    "        if i == 0:\n",
    "            converter = Begining\n",
    "        elif i == len(word) - 1:\n",
    "            converter = Ending\n",
    "        else:\n",
    "            converter = Middle\n",
    "        conversions = converter.get(letter)\n",
    "        if conversions == None:\n",
    "            return [(''.join(original_word), 0.0)]\n",
    "        else:\n",
    "            conversions = ['' if i == 'nothing' else i for i in conversions]\n",
    "        persian.append(conversions)\n",
    "\n",
    "    alternatives = itertools.product(*persian)\n",
    "    alternatives = [''.join(i) for i in alternatives]\n",
    "\n",
    "    alternatives = [(i, WordFreq[i]) if i in WordFreq else (i, 0)\n",
    "                    for i in alternatives]\n",
    "\n",
    "    if len(alternatives) > 0:\n",
    "        max_freq = max(freq for _, freq in alternatives)\n",
    "        alternatives = [(w, float(freq / max_freq)) if freq != 0 else (w, 0.0)\n",
    "                        for w, freq in alternatives]\n",
    "    else:\n",
    "        alternatives = [(''.join(word), 1.0)]\n",
    "\n",
    "    return alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variations(word):\n",
    "    \"\"\"Create variations of the word based on letter combinations like oo,\n",
    "sh, etc.\"\"\"\n",
    "\n",
    "    if word == 'a':\n",
    "        return [['A']]\n",
    "    elif len(word) == 1:\n",
    "        return [[word[0]]]\n",
    "    elif word == 'aa':\n",
    "        return [['A']]\n",
    "    elif word == 'ee':\n",
    "        return [['i']]\n",
    "    elif word == 'ei':\n",
    "        return [['ei']]\n",
    "    elif word in ['oo', 'ou']:\n",
    "        return [['u']]\n",
    "    elif word == 'kha':\n",
    "        return [['kha'], ['kh', 'a']]\n",
    "    elif word in ['kh', 'gh', 'ch', 'sh', 'zh', 'ck']:\n",
    "        return [[word]]\n",
    "    elif word in [\"'ee\", \"'ei\"]:\n",
    "        return [[\"'i\"]]\n",
    "    elif word in [\"'oo\", \"'ou\"]:\n",
    "        return [[\"'u\"]]\n",
    "    elif word in [\"a'\", \"e'\", \"o'\", \"i'\", \"u'\", \"A'\"]:\n",
    "        return [[word[0] + \"'\"]]\n",
    "    elif word in [\"'a\", \"'e\", \"'o\", \"'i\", \"'u\", \"'A\"]:\n",
    "        return [[\"'\" + word[1]]]\n",
    "    elif len(word) == 2 and word[0] == word[1]:\n",
    "        return [[word[0]]]\n",
    "\n",
    "    if word[:2] == 'aa':\n",
    "        return [['A'] + i for i in variations(word[2:])]\n",
    "    elif word[:2] == 'ee':\n",
    "        return [['i'] + i for i in variations(word[2:])]\n",
    "    elif word[:2] in ['oo', 'ou']:\n",
    "        return [['u'] + i for i in variations(word[2:])]\n",
    "    elif word[:3] == 'kha':\n",
    "        return \\\n",
    "            [['kha'] + i for i in variations(word[3:])] + \\\n",
    "            [['kh', 'a'] + i for i in variations(word[3:])] + \\\n",
    "            [['k', 'h', 'a'] + i for i in variations(word[3:])]\n",
    "    elif word[:2] in ['kh', 'gh', 'ch', 'sh', 'zh', 'ck']:\n",
    "        return \\\n",
    "            [[word[:2]] + i for i in variations(word[2:])] + \\\n",
    "            [[word[0]] + i for i in variations(word[1:])]\n",
    "    elif word[:2] in [\"a'\", \"e'\", \"o'\", \"i'\", \"u'\", \"A'\"]:\n",
    "        return [[word[:2]] + i for i in variations(word[2:])]\n",
    "    elif word[:3] in [\"'ee\", \"'ei\"]:\n",
    "        return [[\"'i\"] + i for i in variations(word[3:])]\n",
    "    elif word[:3] in [\"'oo\", \"'ou\"]:\n",
    "        return [[\"'u\"] + i for i in variations(word[3:])]\n",
    "    elif word[:2] in [\"'a\", \"'e\", \"'o\", \"'i\", \"'u\", \"'A\"]:\n",
    "        return [[word[:2]] + i for i in variations(word[2:])]\n",
    "    elif len(word) >= 2 and word[0] == word[1]:\n",
    "        return [[word[0]] + i for i in variations(word[2:])]\n",
    "    else:\n",
    "        return [[word[0]] + i for i in variations(word[1:])]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_word(word, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a single word from Finglish to Persian.\n",
    "\n",
    "    max_word_size: Maximum size of the words to consider. Words larger\n",
    "    than this will be kept unchanged.\n",
    "\n",
    "    cutoff: The cut-off point. For each word, there could be many\n",
    "    possibilities. By default 3 of these possibilities are considered\n",
    "    for each word. This number can be changed by this argument.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    original_word = word\n",
    "    word = word.lower()\n",
    "\n",
    "    c = Dictionary.get(word)\n",
    "    if c:\n",
    "        return [(c, 1.0)]\n",
    "\n",
    "    if word == '':\n",
    "        return []\n",
    "    elif len(word) > max_word_size:\n",
    "        return [(original_word, 1.0)]\n",
    "\n",
    "    results = []\n",
    "    for w in variations(word):\n",
    "        #print(w)\n",
    "        results.extend(f2p_word_internal(w, original_word))\n",
    "\n",
    "    # sort results based on the confidence value\n",
    "    results.sort(key=lambda r: r[1], reverse=True)\n",
    "\n",
    "    # return the top three results in order to cut down on the number\n",
    "    # of possibilities.\n",
    "    return results[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_list(phrase, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a phrase from Finglish to Persian.\n",
    "\n",
    "    phrase: The phrase to convert.\n",
    "\n",
    "    max_word_size: Maximum size of the words to consider. Words larger\n",
    "    than this will be kept unchanged.\n",
    "\n",
    "    cutoff: The cut-off point. For each word, there could be many\n",
    "    possibilities. By default 3 of these possibilities are considered\n",
    "    for each word. This number can be changed by this argument.\n",
    "\n",
    "    Returns a list of lists, each sub-list contains a number of\n",
    "    possibilities for each word as a pair of (word, confidence)\n",
    "    values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # split the phrase into words\n",
    "    results = [w for w in sep_regex.split(phrase) if w]\n",
    "\n",
    "    # return an empty list if no words\n",
    "    if results == []:\n",
    "        return []\n",
    "\n",
    "    # convert each word separately\n",
    "    results = [f2p_word(w, max_word_size, cutoff) for w in results]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p(phrase, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a Finglish phrase to the most probable Persian phrase.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    results = f2p_list(phrase, max_word_size, cutoff)\n",
    "    #for i in results:\n",
    "    #    print(i)\n",
    "    return ' '.join(i[0][0] for i in results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 تا یار برای ولیبل احتیاج داریم سات 2 سالن نوبخت\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"2 ta yar baraye volleyball ehtiaj darim saat 2 salon nobakht\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سات\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"saat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لتفن\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"lotfan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چراغ\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"cheraq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کردهیم\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"kardehim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خواهر\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"khahar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "با ظهور مدلهای زبانی بزرگ میتوان در زمان کم خروجی مناسب دریافت کرد\n",
      "با ظهور مدل‌های زبانی بزرگ می توان در زمان کم خروجی مناسب دریافت کرد.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Ba zohoore modelhaye zabani bozorg mitavan dar zamane kam khorooji monaseb daryaft kard.\"\n",
    "output_text = \"با ظهور مدل\\u200cهای زبانی بزرگ می توان در زمان کم خروجی مناسب دریافت کرد.\"\n",
    "\n",
    "result = f2p(input_text)\n",
    "print(result)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "میشه این فارسی کنی\n"
     ]
    }
   ],
   "source": [
    "text = \"misheh ino farsi koni?\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اینکه اشکال در\n"
     ]
    }
   ],
   "source": [
    "text = \"inke eshkal dare!\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "این پر کونین لتفن\n"
     ]
    }
   ],
   "source": [
    "text = \"Ino por konin lotfan\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اگه امشب نش تا فردا حتماً میش\n"
     ]
    }
   ],
   "source": [
    "text = \"age emshab nashe ta farada hatman mishe.\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سبز سفید قرمز\n"
     ]
    }
   ],
   "source": [
    "text = \"sabzo sefido ghermez\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "البته فک نکونام این طور باش\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"Albte fk nakonam in tor bashe.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingilish_sentences = [\n",
    "    \"Emrooz hava besyar delpazir bood.\",\n",
    "    \"Madaram ghaza-ye khoshmaze-i baraye sham dorost kardeh ast.\",\n",
    "    \"Farda be kuhpeymayi khahim raft.\",\n",
    "    \"Ketabe jadidi ke kharideham besyar jaleb ast.\",\n",
    "    \"Doost daram vaghti hava abri ast be park beravam.\",\n",
    "    \"Khaneman ra taze rang kardehim.\",\n",
    "    \"Har rooz sobh yek fenjan chai minusham.\",\n",
    "    \"Gorbehaman asheghe bazi ba kamva ast.\",\n",
    "    \"Pedaram hamishe mara be varzesh tashvigh mikonad.\",\n",
    "    \"Hafte ayande be didare doostanam miravam.\",\n",
    "    \"Yadgiriye zabane jadid mitavanad tajrobe jazabi bashad.\",\n",
    "    \"Emrooz ba khanevade be sinema raftim.\",\n",
    "    \"Man be golhaye hayatman rasidgi mikonam.\",\n",
    "    \"Har shab ghabl az khob kami motalee mikonam.\",\n",
    "    \"Emtehan hafte ayande kheyli mohem ast.\",\n",
    "    \"Az tamashaye ghorube aftab lezzat mibaram.\",\n",
    "    \"Behtarin zaman baraye piaderavi sobh zood ast.\",\n",
    "    \"Khaharam naghashihaye zibai mikeshad.\",\n",
    "    \"Karhaye honari ra kheyli doost daram.\",\n",
    "    \"Baraye rooze tavallode doostam yek hediye aali kharidam.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_sentences = [\n",
    "    \"امروز هوا بسیار دلپذیر بود.\",\n",
    "    \"مادرم غذای خوشمزه‌ای برای شام درست کرده است.\",\n",
    "    \"فردا به کوه‌پیمایی خواهیم رفت.\",\n",
    "    \"کتاب جدیدی که خریده‌ام بسیار جالب است.\",\n",
    "    \"دوست دارم وقتی هوا ابری است به پارک بروم.\",\n",
    "    \"خانه‌مان را تازه رنگ کرده‌ایم.\",\n",
    "    \"هر روز صبح یک فنجان چای می‌نوشم.\",\n",
    "    \"گربه‌مان عاشق بازی با کاموا است.\",\n",
    "    \"پدرم همیشه مرا به ورزش تشویق می‌کند.\",\n",
    "    \"هفته آینده به دیدار دوستانم می‌روم.\",\n",
    "    \"یادگیری زبان جدید می‌تواند تجربه جذابی باشد.\",\n",
    "    \"امروز با خانواده به سینما رفتیم.\",\n",
    "    \"من به گل‌های حیاط‌مان رسیدگی می‌کنم.\",\n",
    "    \"هر شب قبل از خواب کمی مطالعه می‌کنم.\",\n",
    "    \"امتحان هفته آینده خیلی مهم است.\",\n",
    "    \"از تماشای غروب آفتاب لذت می‌برم.\",\n",
    "    \"بهترین زمان برای پیاده‌روی صبح زود است.\",\n",
    "    \"خواهرم نقاشی‌های زیبایی می‌کشد.\",\n",
    "    \"کارهای هنری را خیلی دوست دارم.\",\n",
    "    \"برای روز تولد دوستم یک هدیه عالی خریدم.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "امروز هوا بسیار دلپذیر بود\n",
      "امروز هوا بسیار دلپذیر بود.\n",
      "##############################\n",
      "مادرم غذا یه خوشمزه ای برای شام درست کرده است\n",
      "مادرم غذای خوشمزه‌ای برای شام درست کرده است.\n",
      "##############################\n",
      "فردا به کوهپیمایی خواهیم رفت\n",
      "فردا به کوه‌پیمایی خواهیم رفت.\n",
      "##############################\n",
      "کتاب جدیدی که خاریدهم بسیار جلب است\n",
      "کتاب جدیدی که خریده‌ام بسیار جالب است.\n",
      "##############################\n",
      "دوست دارم وقتی هوا عبری است به پارک بروم\n",
      "دوست دارم وقتی هوا ابری است به پارک بروم.\n",
      "##############################\n",
      "خانمان را تازه رنگ کردهیم\n",
      "خانه‌مان را تازه رنگ کرده‌ایم.\n",
      "##############################\n",
      "هر روز صبح یک فنجان چای مینوشم\n",
      "هر روز صبح یک فنجان چای می‌نوشم.\n",
      "##############################\n",
      "گربهمن عشق بازی با کاموا است\n",
      "گربه‌مان عاشق بازی با کاموا است.\n",
      "##############################\n",
      "پدرم همیشه مرا به ورزش تشویق میکند\n",
      "پدرم همیشه مرا به ورزش تشویق می‌کند.\n",
      "##############################\n",
      "هفت آینده به دیدار دوستانم میروم\n",
      "هفته آینده به دیدار دوستانم می‌روم.\n",
      "##############################\n",
      "یدگیرییع زبان جدید میتواند تجربه جذبی باشد\n",
      "یادگیری زبان جدید می‌تواند تجربه جذابی باشد.\n",
      "##############################\n",
      "امروز با خانواده به سینما رفتیم\n",
      "امروز با خانواده به سینما رفتیم.\n",
      "##############################\n",
      "من به گلهای هیتمن رسیدگی میکنم\n",
      "من به گل‌های حیاط‌مان رسیدگی می‌کنم.\n",
      "##############################\n",
      "هر شب قبل از خوب کمی متالی میکنم\n",
      "هر شب قبل از خواب کمی مطالعه می‌کنم.\n",
      "##############################\n",
      "امتحان هفت آینده خیلی مهم است\n",
      "امتحان هفته آینده خیلی مهم است.\n",
      "##############################\n",
      "از تماشای غروب آفتاب لذت میبرم\n",
      "از تماشای غروب آفتاب لذت می‌برم.\n",
      "##############################\n",
      "بهترین زمان برای پیدروی صبح زود است\n",
      "بهترین زمان برای پیاده‌روی صبح زود است.\n",
      "##############################\n",
      "خواهرم نقاشیهای زیبای میکشد\n",
      "خواهرم نقاشی‌های زیبایی می‌کشد.\n",
      "##############################\n",
      "کارهای هنری را خیلی دوست دارم\n",
      "کارهای هنری را خیلی دوست دارم.\n",
      "##############################\n",
      "برای روز تولد دوستم یک هدییع عالی خریدم\n",
      "برای روز تولد دوستم یک هدیه عالی خریدم.\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "for f_sentence, p_sentence in zip(fingilish_sentences, persian_sentences):\n",
    "    print(f2p(f_sentence))\n",
    "    print(p_sentence)\n",
    "    print(\"#\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
