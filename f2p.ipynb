{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary = {}\n",
    "Begining = {}\n",
    "Middle = {}\n",
    "Ending = {}\n",
    "WordFreq = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 699\n"
     ]
    }
   ],
   "source": [
    "dict_file = open('dict.txt', 'r')\n",
    "dict_Lines = dict_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in dict_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    if len(splited) != 2: continue\n",
    "    key = splited[0]\n",
    "    value = splited[1]\n",
    "    Dictionary[key] = value\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "توکیو\n",
      "وی‌او‌ای\n",
      "سس\n",
      "زهرا\n"
     ]
    }
   ],
   "source": [
    "print(Dictionary[\"tokyo\"])\n",
    "print(Dictionary[\"voa\"])\n",
    "print(Dictionary[\"sos\"])\n",
    "print(Dictionary[\"zahra\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 38\n"
     ]
    }
   ],
   "source": [
    "beg_file = open('beginning.txt', 'r')\n",
    "beg_Lines = beg_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in beg_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Begining[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['خا', 'خوا', 'خ']\n",
      "['ش']\n",
      "['ج', 'ژ']\n",
      "['چ']\n"
     ]
    }
   ],
   "source": [
    "print(Begining[\"kha\"])\n",
    "print(Begining[\"sh\"])\n",
    "print(Begining[\"j\"])\n",
    "print(Begining[\"ch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 45\n"
     ]
    }
   ],
   "source": [
    "mid_file = open('middle.txt', 'r')\n",
    "mid_Lines = mid_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in mid_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Middle[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ش']\n",
      "['ی', 'یع', 'ئی', 'عی']\n",
      "['ک']\n",
      "['nothing', 'ع', 'ا', 'اع', 'أ', 'ؤ', 'ؤا', 'ئا', 'ئ']\n"
     ]
    }
   ],
   "source": [
    "print(Middle[\"sh\"])\n",
    "print(Middle[\"i\"])\n",
    "print(Middle[\"ck\"])\n",
    "print(Middle[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 48\n"
     ]
    }
   ],
   "source": [
    "end_file = open('ending.txt', 'r')\n",
    "end_Lines = end_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in end_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Ending[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ش']\n",
      "['ی', 'یع']\n",
      "['ک']\n",
      "['ا', 'اع', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "print(Ending[\"sh\"])\n",
    "print(Ending[\"i\"])\n",
    "print(Ending[\"ck\"])\n",
    "print(Ending[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.WordFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 111302\n"
     ]
    }
   ],
   "source": [
    "freq_file = open('persian-wikipedia.txt', 'r')\n",
    "freq_Lines = freq_file.readlines()\n",
    " \n",
    "min_freq = 4\n",
    "count = 0\n",
    "for line in freq_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\"\\t\")\n",
    "    word = splited[0]\n",
    "    freq = int(splited[1]) \n",
    "    if freq < min_freq:\n",
    "        break\n",
    "    if word not in WordFreq.keys():\n",
    "        WordFreq[word] = freq\n",
    "\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84193\n",
      "24228\n",
      "5\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "print(WordFreq[\"یک\"])\n",
    "print(WordFreq[\"جمعیت\"])\n",
    "print(WordFreq[\"گودترین\"])\n",
    "print(WordFreq[\"لطفا\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "sep_regex = re.compile(r'[ \\-_~!@#%$^&*\\(\\)\\[\\]\\{\\}/\\:;\"|,./?`]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_word_internal(word, original_word):\n",
    "    # this function receives the word as separate letters\n",
    "    persian = []\n",
    "    for i, letter in enumerate(word):\n",
    "        if i == 0:\n",
    "            converter = Begining\n",
    "        elif i == len(word) - 1:\n",
    "            converter = Ending\n",
    "        else:\n",
    "            converter = Middle\n",
    "        conversions = converter.get(letter)\n",
    "        if conversions == None:\n",
    "            return [(''.join(original_word), 0.0)]\n",
    "        else:\n",
    "            conversions = ['' if i == 'nothing' else i for i in conversions]\n",
    "        persian.append(conversions)\n",
    "\n",
    "    alternatives = itertools.product(*persian)\n",
    "    alternatives = [''.join(i) for i in alternatives]\n",
    "    \n",
    "    \n",
    "    alternatives = [(i, WordFreq[i]) if i in WordFreq else (i, 0) for i in alternatives]\n",
    "    #print(alternatives)\n",
    "    \n",
    "    #result = []\n",
    "    #flag = False\n",
    "    #for i in alternatives:\n",
    "    #    if i in WordFreq:\n",
    "    #       result.append((i, WordFreq[i])) \n",
    "        \n",
    "\n",
    "    #print(alternatives)\n",
    "    if len(alternatives) > 0:\n",
    "        return [max(alternatives, key=lambda x:x[1])]\n",
    "    else:\n",
    "        return [(''.join(word), 1.0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variations(word):\n",
    "    \"\"\"Create variations of the word based on letter combinations like oo,\n",
    "sh, etc.\"\"\"\n",
    "\n",
    "    if word == 'a':\n",
    "        return [['A']]\n",
    "    elif len(word) == 1:\n",
    "        return [[word[0]]]\n",
    "    elif word == 'aa':\n",
    "        return [['A']]\n",
    "    elif word == 'ee':\n",
    "        return [['i']]\n",
    "    elif word == 'iy':\n",
    "        return [['i']]\n",
    "    elif word == 'ei':\n",
    "        return [['ei']]\n",
    "    elif word in ['oo', 'ou']:\n",
    "        return [['u']]\n",
    "    elif word == 'kha':\n",
    "        return [['kha'], ['kh', 'a']]\n",
    "    elif word in ['kh', 'gh', 'ch', 'sh', 'zh', 'ck']:\n",
    "        return [[word]]\n",
    "    elif word in [\"'ee\", \"'ei\"]:\n",
    "        return [[\"'i\"]]\n",
    "    elif word in [\"'oo\", \"'ou\"]:\n",
    "        return [[\"'u\"]]\n",
    "    elif word in [\"a'\", \"e'\", \"o'\", \"i'\", \"u'\", \"A'\"]:\n",
    "        return [[word[0] + \"'\"]]\n",
    "    elif word in [\"'a\", \"'e\", \"'o\", \"'i\", \"'u\", \"'A\"]:\n",
    "        return [[\"'\" + word[1]]]\n",
    "    elif len(word) == 2 and word[0] == word[1]:\n",
    "        return [[word[0]]]\n",
    "\n",
    "    if word[:2] == 'aa':\n",
    "        return [['A'] + i for i in variations(word[2:])] + \\\n",
    "               [['a', 'a'] + i for i in variations(word[2:])]\n",
    "    elif word[:2] in ['ee', 'iy']:\n",
    "        return [['i'] + i for i in variations(word[2:])]\n",
    "    elif word[:2] in ['oo', 'ou']:\n",
    "        return [['u'] + i for i in variations(word[2:])]\n",
    "    elif word[:3] == 'kha':\n",
    "        return \\\n",
    "            [['kha'] + i for i in variations(word[3:])] + \\\n",
    "            [['kh', 'a'] + i for i in variations(word[3:])] + \\\n",
    "            [['k', 'h', 'a'] + i for i in variations(word[3:])]\n",
    "    elif word[:2] in ['kh', 'gh', 'ch', 'sh', 'zh', 'ck']:\n",
    "        return \\\n",
    "            [[word[:2]] + i for i in variations(word[2:])] \n",
    "           # [[word[0]] + i for i in variations(word[1:])]\n",
    "    elif word[:2] in [\"a'\", \"e'\", \"o'\", \"i'\", \"u'\", \"A'\"]:\n",
    "        return [[word[:2]] + i for i in variations(word[2:])]\n",
    "    elif word[:3] in [\"'ee\", \"'ei\"]:\n",
    "        return [[\"'i\"] + i for i in variations(word[3:])]\n",
    "    elif word[:3] in [\"'oo\", \"'ou\"]:\n",
    "        return [[\"'u\"] + i for i in variations(word[3:])]\n",
    "    elif word[:2] in [\"'a\", \"'e\", \"'o\", \"'i\", \"'u\", \"'A\"]:\n",
    "        return [[word[:2]] + i for i in variations(word[2:])]\n",
    "    elif len(word) >= 2 and word[0] == word[1]:\n",
    "        return [[word[0]] + i for i in variations(word[2:])]\n",
    "    else:\n",
    "        return [[word[0]] + i for i in variations(word[1:])]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_word(word, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a single word from Finglish to Persian.\n",
    "\n",
    "    max_word_size: Maximum size of the words to consider. Words larger\n",
    "    than this will be kept unchanged.\n",
    "\n",
    "    cutoff: The cut-off point. For each word, there could be many\n",
    "    possibilities. By default 3 of these possibilities are considered\n",
    "    for each word. This number can be changed by this argument.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    original_word = word\n",
    "    word = word.lower()\n",
    "\n",
    "    c = Dictionary.get(word)\n",
    "    if c:\n",
    "        return c\n",
    "\n",
    "    if word == '':\n",
    "        return []\n",
    "    elif len(word) > max_word_size:\n",
    "        return [(original_word, 1.0)]\n",
    "\n",
    "    results = []\n",
    "    for w in variations(word):\n",
    "        #print(w)\n",
    "        results.extend(f2p_word_internal(w, original_word))\n",
    "    \n",
    "    return max(results, key=lambda x:x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_list(phrase, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a phrase from Finglish to Persian.\n",
    "\n",
    "    phrase: The phrase to convert.\n",
    "\n",
    "    max_word_size: Maximum size of the words to consider. Words larger\n",
    "    than this will be kept unchanged.\n",
    "\n",
    "    cutoff: The cut-off point. For each word, there could be many\n",
    "    possibilities. By default 3 of these possibilities are considered\n",
    "    for each word. This number can be changed by this argument.\n",
    "\n",
    "    Returns a list of lists, each sub-list contains a number of\n",
    "    possibilities for each word as a pair of (word, confidence)\n",
    "    values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # split the phrase into words\n",
    "    results = [w for w in sep_regex.split(phrase) if w]\n",
    "\n",
    "    # return an empty list if no words\n",
    "    if results == []:\n",
    "        return []\n",
    "\n",
    "    # convert each word separately\n",
    "    results = [f2p_word(w, max_word_size, cutoff) for w in results]\n",
    "    #print(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p(phrase, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a Finglish phrase to the most probable Persian phrase.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    results = f2p_list(phrase, max_word_size, cutoff)\n",
    "    #print(results)\n",
    "    #for i in results:\n",
    "    #    print(i)\n",
    "    return ' '.join(i for i in results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هدیه\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"hediye\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "داره\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"dareh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اینو\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"ino\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لطفا\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"lotfan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چراغ\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"cheraq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کرده ایم\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"kardeh im\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خواب\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"khab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عبری\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"ebri\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ابری\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"abri\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "با ظهور مدلهای زبانی بزرگ میتوان در زمان کم خروجی مناسب دریافت کرد\n",
      "با ظهور مدل‌های زبانی بزرگ می توان در زمان کم خروجی مناسب دریافت کرد.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Ba zohoore modelhaye zabani bozorg mitavan dar zamane kam khorooji monaseb daryaft kard.\"\n",
    "output_text = \"با ظهور مدل\\u200cهای زبانی بزرگ می توان در زمان کم خروجی مناسب دریافت کرد.\"\n",
    "\n",
    "result = f2p(input_text)\n",
    "print(result)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "میشه اینو فارسی کنی\n"
     ]
    }
   ],
   "source": [
    "text = \"misheh ino farsi koni?\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اینکه اشکال داره\n"
     ]
    }
   ],
   "source": [
    "text = \"inke eshkal dare!\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اینو پر کنین لطفا\n"
     ]
    }
   ],
   "source": [
    "text = \"Ino por konin lotfan\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اگه امشب نشه تا فردا حتماً میشه\n"
     ]
    }
   ],
   "source": [
    "text = \"age emshab nashe ta farada hatman mishe.\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سبز و سفید و قرمز\n"
     ]
    }
   ],
   "source": [
    "text = \"sabz o sefid o ghermez\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "البته فک نکن ام این طور باشه\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"Albte fk nakon am in tor bashe.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 تا یار برای والیبال احتیاج داریم ساعت 2 سالن نوبخت\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"2 ta yar baraye valibal ehtiaj darim saat 2 salon nobakht\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingilish_sentences = [\n",
    "    \"Emrooz hava besyar delpazir bood.\",\n",
    "    \"Madaram ghaza-ye khoshmaze-i baraye sham dorost kardeh ast.\",\n",
    "    \"Farda be kuhpeymayi khahim raft.\",\n",
    "    \"Ketabe jadidi ke kharideham besyar jaleb ast.\",\n",
    "    \"Doost daram vaghti hava abri ast be park beravam.\",\n",
    "    \"Khaneman ra taze rang kardehim.\",\n",
    "    \"Har rooz sobh yek fenjan chai minusham.\",\n",
    "    \"Gorbehaman asheghe bazi ba kamva ast.\",\n",
    "    \"Pedaram hamishe mara be varzesh tashvigh mikonad.\",\n",
    "    \"Hafte ayande be didare doostanam miravam.\",\n",
    "    \"Yadgiriye zabane jadid mitavanad tajrobe jazabi bashad.\",\n",
    "    \"Emrooz ba khanevade be sinema raftim.\",\n",
    "    \"Man be golhaye hayatman rasidgi mikonam.\",\n",
    "    \"Har shab ghabl az khab kami motalee mikonam.\",\n",
    "    \"Emtehan hafte ayande kheyli mohem ast.\",\n",
    "    \"Az tamashaye ghorube aftab lezzat mibaram.\",\n",
    "    \"Behtarin zaman baraye piaderavi sobh zood ast.\",\n",
    "    \"Khaharam naghashihaye zibai mikeshad.\",\n",
    "    \"Karhaye honari ra kheyli doost daram.\",\n",
    "    \"Baraye rooze tavallode doostam yek hediye aali kharidam.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_sentences = [\n",
    "    \"امروز هوا بسیار دلپذیر بود.\",\n",
    "    \"مادرم غذای خوشمزه‌ای برای شام درست کرده است.\",\n",
    "    \"فردا به کوه‌پیمایی خواهیم رفت.\",\n",
    "    \"کتاب جدیدی که خریده‌ام بسیار جالب است.\",\n",
    "    \"دوست دارم وقتی هوا ابری است به پارک بروم.\",\n",
    "    \"خانه‌مان را تازه رنگ کرده‌ایم.\",\n",
    "    \"هر روز صبح یک فنجان چای می‌نوشم.\",\n",
    "    \"گربه‌مان عاشق بازی با کاموا است.\",\n",
    "    \"پدرم همیشه مرا به ورزش تشویق می‌کند.\",\n",
    "    \"هفته آینده به دیدار دوستانم می‌روم.\",\n",
    "    \"یادگیری زبان جدید می‌تواند تجربه جذابی باشد.\",\n",
    "    \"امروز با خانواده به سینما رفتیم.\",\n",
    "    \"من به گل‌های حیاط‌مان رسیدگی می‌کنم.\",\n",
    "    \"هر شب قبل از خواب کمی مطالعه می‌کنم.\",\n",
    "    \"امتحان هفته آینده خیلی مهم است.\",\n",
    "    \"از تماشای غروب آفتاب لذت می‌برم.\",\n",
    "    \"بهترین زمان برای پیاده‌روی صبح زود است.\",\n",
    "    \"خواهرم نقاشی‌های زیبایی می‌کشد.\",\n",
    "    \"کارهای هنری را خیلی دوست دارم.\",\n",
    "    \"برای روز تولد دوستم یک هدیه عالی خریدم.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "امروز هوا بسیار دلپذیر بود\n",
      "امروز هوا بسیار دلپذیر بود.\n",
      "##############################\n",
      "مادرم غذا یه خوشمزه ای برای شام درست کرده است\n",
      "مادرم غذای خوشمزه‌ای برای شام درست کرده است.\n",
      "##############################\n",
      "فردا به کوهپیمایی خواهیم رفت\n",
      "فردا به کوه‌پیمایی خواهیم رفت.\n",
      "##############################\n",
      "کتاب جدیدی که خاریدهم بسیار جلب است\n",
      "کتاب جدیدی که خریده‌ام بسیار جالب است.\n",
      "##############################\n",
      "دوست دارم وقتی هوا ابری است به پارک بروم\n",
      "دوست دارم وقتی هوا ابری است به پارک بروم.\n",
      "##############################\n",
      "خانمان را تازه رنگ کردهیم\n",
      "خانه‌مان را تازه رنگ کرده‌ایم.\n",
      "##############################\n",
      "هر روز صبح یک فنجان چای مینوشم\n",
      "هر روز صبح یک فنجان چای می‌نوشم.\n",
      "##############################\n",
      "گربهمن عاشق بازی با کاموا است\n",
      "گربه‌مان عاشق بازی با کاموا است.\n",
      "##############################\n",
      "پدرم همیشه مرا به ورزش تشویق میکند\n",
      "پدرم همیشه مرا به ورزش تشویق می‌کند.\n",
      "##############################\n",
      "هفته آینده به دیدار دوستانم میروم\n",
      "هفته آینده به دیدار دوستانم می‌روم.\n",
      "##############################\n",
      "یادگیری زبان جدید میتواند تجربه جذبی باشد\n",
      "یادگیری زبان جدید می‌تواند تجربه جذابی باشد.\n",
      "##############################\n",
      "امروز با خانواده به سینما رفتیم\n",
      "امروز با خانواده به سینما رفتیم.\n",
      "##############################\n",
      "من به گلهای هیتمن رسیدگی میکنم\n",
      "من به گل‌های حیاط‌مان رسیدگی می‌کنم.\n",
      "##############################\n",
      "هر شب قبل از خواب کمی متالی میکنم\n",
      "هر شب قبل از خواب کمی مطالعه می‌کنم.\n",
      "##############################\n",
      "امتحان هفته آینده خیلی مهم است\n",
      "امتحان هفته آینده خیلی مهم است.\n",
      "##############################\n",
      "از تماشای غروب آفتاب لذت میبرم\n",
      "از تماشای غروب آفتاب لذت می‌برم.\n",
      "##############################\n",
      "بهترین زمان برای پیدروی صبح زود است\n",
      "بهترین زمان برای پیاده‌روی صبح زود است.\n",
      "##############################\n",
      "خواهرم نقاشیهای زیبای میکشد\n",
      "خواهرم نقاشی‌های زیبایی می‌کشد.\n",
      "##############################\n",
      "کارهای هنری را خیلی دوست دارم\n",
      "کارهای هنری را خیلی دوست دارم.\n",
      "##############################\n",
      "برای روز تولد دوستم یک هدیه عالی خریدم\n",
      "برای روز تولد دوستم یک هدیه عالی خریدم.\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "for f_sentence, p_sentence in zip(fingilish_sentences, persian_sentences):\n",
    "    print(f2p(f_sentence))\n",
    "    print(p_sentence)\n",
    "    print(\"#\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
