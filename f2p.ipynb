{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary = {}\n",
    "Begining = {}\n",
    "Middle = {}\n",
    "Ending = {}\n",
    "WordFreq = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 689\n"
     ]
    }
   ],
   "source": [
    "dict_file = open('dict.txt', 'r')\n",
    "dict_Lines = dict_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in dict_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    if len(splited) != 2: continue\n",
    "    key = splited[0]\n",
    "    value = splited[1]\n",
    "    Dictionary[key] = value\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "توکیو\n",
      "وی‌او‌ای\n",
      "سس\n",
      "زهرا\n"
     ]
    }
   ],
   "source": [
    "print(Dictionary[\"tokyo\"])\n",
    "print(Dictionary[\"voa\"])\n",
    "print(Dictionary[\"sos\"])\n",
    "print(Dictionary[\"zahra\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 38\n"
     ]
    }
   ],
   "source": [
    "beg_file = open('beginning.txt', 'r')\n",
    "beg_Lines = beg_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in beg_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Begining[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['خا', 'خوا', 'خ']\n",
      "['ش']\n",
      "['ج', 'ژ']\n",
      "['چ']\n"
     ]
    }
   ],
   "source": [
    "print(Begining[\"kha\"])\n",
    "print(Begining[\"sh\"])\n",
    "print(Begining[\"j\"])\n",
    "print(Begining[\"ch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 45\n"
     ]
    }
   ],
   "source": [
    "mid_file = open('middle.txt', 'r')\n",
    "mid_Lines = mid_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in mid_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Middle[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ش']\n",
      "['ی', 'یع', 'ئی', 'عی']\n",
      "['ک']\n",
      "['nothing', 'ع', 'ا', 'اع', 'أ', 'ؤ', 'ؤا', 'ئا', 'ئ']\n"
     ]
    }
   ],
   "source": [
    "print(Middle[\"sh\"])\n",
    "print(Middle[\"i\"])\n",
    "print(Middle[\"ck\"])\n",
    "print(Middle[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 47\n"
     ]
    }
   ],
   "source": [
    "end_file = open('ending.txt', 'r')\n",
    "end_Lines = end_file.readlines()\n",
    " \n",
    "count = 0\n",
    "for line in end_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\" \")\n",
    "    key = splited[0]\n",
    "    values = splited[1:]\n",
    "    Ending[key] = values\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ش']\n",
      "['ی', 'یع']\n",
      "['ک']\n",
      "['ا', 'اع', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "print(Ending[\"sh\"])\n",
    "print(Ending[\"i\"])\n",
    "print(Ending[\"ck\"])\n",
    "print(Ending[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.WordFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 111302\n"
     ]
    }
   ],
   "source": [
    "freq_file = open('persian-wikipedia.txt', 'r')\n",
    "freq_Lines = freq_file.readlines()\n",
    " \n",
    "min_freq = 4\n",
    "count = 0\n",
    "for line in freq_Lines:\n",
    "    count += 1\n",
    "    splited = line[:-1].split(\"\\t\")\n",
    "    word = splited[0]\n",
    "    freq = int(splited[1]) \n",
    "    if freq < min_freq:\n",
    "        break\n",
    "    if word not in WordFreq.keys():\n",
    "        WordFreq[word] = freq\n",
    "\n",
    "    \n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84193\n",
      "24228\n",
      "5\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "print(WordFreq[\"یک\"])\n",
    "print(WordFreq[\"جمعیت\"])\n",
    "print(WordFreq[\"گودترین\"])\n",
    "print(WordFreq[\"لطفا\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "sep_regex = re.compile(r'[ \\-_~!@#%$^&*\\(\\)\\[\\]\\{\\}/\\:;\"|,./?`]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_word_internal(word, original_word):\n",
    "    # this function receives the word as separate letters\n",
    "    persian = []\n",
    "    for i, letter in enumerate(word):\n",
    "        if i == 0:\n",
    "            converter = Begining\n",
    "        elif i == len(word) - 1:\n",
    "            converter = Ending\n",
    "        else:\n",
    "            converter = Middle\n",
    "        conversions = converter.get(letter)\n",
    "        if conversions == None:\n",
    "            return [(''.join(original_word), 0.0)]\n",
    "        else:\n",
    "            conversions = ['' if i == 'nothing' else i for i in conversions]\n",
    "        persian.append(conversions)\n",
    "\n",
    "    alternatives = itertools.product(*persian)\n",
    "    alternatives = [''.join(i) for i in alternatives]\n",
    "\n",
    "    alternatives = [(i, WordFreq[i]) if i in WordFreq else (i, 0)\n",
    "                    for i in alternatives]\n",
    "\n",
    "    if len(alternatives) > 0:\n",
    "        max_freq = max(freq for _, freq in alternatives)\n",
    "        alternatives = [(w, float(freq / max_freq)) if freq != 0 else (w, 0.0)\n",
    "                        for w, freq in alternatives]\n",
    "    else:\n",
    "        alternatives = [(''.join(word), 1.0)]\n",
    "\n",
    "    return alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variations(word):\n",
    "    \"\"\"Create variations of the word based on letter combinations like oo,\n",
    "sh, etc.\"\"\"\n",
    "\n",
    "    if word == 'a':\n",
    "        return [['A']]\n",
    "    elif len(word) == 1:\n",
    "        return [[word[0]]]\n",
    "    elif word == 'aa':\n",
    "        return [['A']]\n",
    "    elif word == 'ee':\n",
    "        return [['i']]\n",
    "    elif word == 'ei':\n",
    "        return [['ei']]\n",
    "    elif word in ['oo', 'ou']:\n",
    "        return [['u']]\n",
    "    elif word == 'kha':\n",
    "        return [['kha'], ['kh', 'a']]\n",
    "    elif word in ['kh', 'gh', 'ch', 'sh', 'zh', 'ck']:\n",
    "        return [[word]]\n",
    "    elif word in [\"'ee\", \"'ei\"]:\n",
    "        return [[\"'i\"]]\n",
    "    elif word in [\"'oo\", \"'ou\"]:\n",
    "        return [[\"'u\"]]\n",
    "    elif word in [\"a'\", \"e'\", \"o'\", \"i'\", \"u'\", \"A'\"]:\n",
    "        return [[word[0] + \"'\"]]\n",
    "    elif word in [\"'a\", \"'e\", \"'o\", \"'i\", \"'u\", \"'A\"]:\n",
    "        return [[\"'\" + word[1]]]\n",
    "    elif len(word) == 2 and word[0] == word[1]:\n",
    "        return [[word[0]]]\n",
    "\n",
    "    if word[:2] == 'aa':\n",
    "        return [['A'] + i for i in variations(word[2:])]\n",
    "    elif word[:2] == 'ee':\n",
    "        return [['i'] + i for i in variations(word[2:])]\n",
    "    elif word[:2] in ['oo', 'ou']:\n",
    "        return [['u'] + i for i in variations(word[2:])]\n",
    "    elif word[:3] == 'kha':\n",
    "        return \\\n",
    "            [['kha'] + i for i in variations(word[3:])] + \\\n",
    "            [['kh', 'a'] + i for i in variations(word[3:])] + \\\n",
    "            [['k', 'h', 'a'] + i for i in variations(word[3:])]\n",
    "    elif word[:2] in ['kh', 'gh', 'ch', 'sh', 'zh', 'ck']:\n",
    "        return \\\n",
    "            [[word[:2]] + i for i in variations(word[2:])] + \\\n",
    "            [[word[0]] + i for i in variations(word[1:])]\n",
    "    elif word[:2] in [\"a'\", \"e'\", \"o'\", \"i'\", \"u'\", \"A'\"]:\n",
    "        return [[word[:2]] + i for i in variations(word[2:])]\n",
    "    elif word[:3] in [\"'ee\", \"'ei\"]:\n",
    "        return [[\"'i\"] + i for i in variations(word[3:])]\n",
    "    elif word[:3] in [\"'oo\", \"'ou\"]:\n",
    "        return [[\"'u\"] + i for i in variations(word[3:])]\n",
    "    elif word[:2] in [\"'a\", \"'e\", \"'o\", \"'i\", \"'u\", \"'A\"]:\n",
    "        return [[word[:2]] + i for i in variations(word[2:])]\n",
    "    elif len(word) >= 2 and word[0] == word[1]:\n",
    "        return [[word[0]] + i for i in variations(word[2:])]\n",
    "    else:\n",
    "        return [[word[0]] + i for i in variations(word[1:])]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_word(word, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a single word from Finglish to Persian.\n",
    "\n",
    "    max_word_size: Maximum size of the words to consider. Words larger\n",
    "    than this will be kept unchanged.\n",
    "\n",
    "    cutoff: The cut-off point. For each word, there could be many\n",
    "    possibilities. By default 3 of these possibilities are considered\n",
    "    for each word. This number can be changed by this argument.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    original_word = word\n",
    "    word = word.lower()\n",
    "\n",
    "    c = Dictionary.get(word)\n",
    "    if c:\n",
    "        return [(c, 1.0)]\n",
    "\n",
    "    if word == '':\n",
    "        return []\n",
    "    elif len(word) > max_word_size:\n",
    "        return [(original_word, 1.0)]\n",
    "\n",
    "    results = []\n",
    "    for w in variations(word):\n",
    "        print(w)\n",
    "        results.extend(f2p_word_internal(w, original_word))\n",
    "\n",
    "    # sort results based on the confidence value\n",
    "    results.sort(key=lambda r: r[1], reverse=True)\n",
    "\n",
    "    # return the top three results in order to cut down on the number\n",
    "    # of possibilities.\n",
    "    return results[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_list(phrase, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a phrase from Finglish to Persian.\n",
    "\n",
    "    phrase: The phrase to convert.\n",
    "\n",
    "    max_word_size: Maximum size of the words to consider. Words larger\n",
    "    than this will be kept unchanged.\n",
    "\n",
    "    cutoff: The cut-off point. For each word, there could be many\n",
    "    possibilities. By default 3 of these possibilities are considered\n",
    "    for each word. This number can be changed by this argument.\n",
    "\n",
    "    Returns a list of lists, each sub-list contains a number of\n",
    "    possibilities for each word as a pair of (word, confidence)\n",
    "    values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # split the phrase into words\n",
    "    results = [w for w in sep_regex.split(phrase) if w]\n",
    "\n",
    "    # return an empty list if no words\n",
    "    if results == []:\n",
    "        return []\n",
    "\n",
    "    # convert each word separately\n",
    "    results = [f2p_word(w, max_word_size, cutoff) for w in results]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p(phrase, max_word_size=15, cutoff=3):\n",
    "    \"\"\"Convert a Finglish phrase to the most probable Persian phrase.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    results = f2p_list(phrase, max_word_size, cutoff)\n",
    "    #for i in results:\n",
    "    #    print(i)\n",
    "    return ' '.join(i[0][0] for i in results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 تا یار برای ولیبل احتیاج داریم سات 2 سالن نوبخت\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"2 ta yar baraye volleyball ehtiaj darim saat 2 salon nobakht\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'A', 't']\n",
      "سات\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"saat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'o', 't', 'f', 'a', 'n']\n",
      "لتفن\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"lotfan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ch', 'e', 'r', 'a', 'q']\n",
      "['c', 'h', 'e', 'r', 'a', 'q']\n",
      "چراغ\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"cheraq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k', 'a', 'r', 'd', 'e', 'h', 'i', 'm']\n",
      "کردهیم\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"kardehim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خواهر\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"khahar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "با ظهور مدلهای زبانی بزرگ میتوان در زمان کم خروجی مناسب دریافت کرد\n",
      "با ظهور مدل‌های زبانی بزرگ می توان در زمان کم خروجی مناسب دریافت کرد.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Ba zohoore modelhaye zabani bozorg mitavan dar zamane kam khorooji monaseb daryaft kard.\"\n",
    "output_text = \"با ظهور مدل\\u200cهای زبانی بزرگ می توان در زمان کم خروجی مناسب دریافت کرد.\"\n",
    "\n",
    "result = f2p(input_text)\n",
    "print(result)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "میشه این فارسی کنی\n"
     ]
    }
   ],
   "source": [
    "text = \"misheh ino farsi koni?\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اینکه اشکال در\n"
     ]
    }
   ],
   "source": [
    "text = \"inke eshkal dare!\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "این پر کونین لتفن\n"
     ]
    }
   ],
   "source": [
    "text = \"Ino por konin lotfan\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اگه امشب نش تا فردا حتماً میش\n"
     ]
    }
   ],
   "source": [
    "text = \"age emshab nashe ta farada hatman mishe.\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سبز سفید قرمز\n"
     ]
    }
   ],
   "source": [
    "text = \"sabzo sefido ghermez\"\n",
    "print(f2p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "البته فک نکونام این طور باش\n"
     ]
    }
   ],
   "source": [
    "print(f2p(\"Albte fk nakonam in tor bashe.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
