{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 1309\n"
     ]
    }
   ],
   "source": [
    "with open('../Irregular Words/irregular_words.txt', 'r') as dict_file:\n",
    "    dictionary = {}\n",
    "    for count, line in enumerate(dict_file, start=1):\n",
    "        splited = line.strip().split(\" \")\n",
    "        if len(splited) == 2:\n",
    "            key, value = splited\n",
    "            dictionary[key] = value\n",
    "\n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "توکیو\n",
      "وی‌او‌ای\n",
      "سس\n",
      "زهرا\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[\"tokyo\"])\n",
    "print(dictionary[\"voa\"])\n",
    "print(dictionary[\"sos\"])\n",
    "print(dictionary[\"zahra\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 38\n"
     ]
    }
   ],
   "source": [
    "with open('../Rules/beginning.txt', 'r') as beg_file:\n",
    "    beginning = {}\n",
    "    for count, line in enumerate(beg_file, start=1):\n",
    "        splited = line.strip().split(\" \")\n",
    "        key = splited[0]\n",
    "        values = splited[1:]\n",
    "        beginning[key] = values\n",
    "\n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['خا', 'خوا', 'خ']\n",
      "['ش']\n",
      "['ج', 'ژ']\n",
      "['چ']\n"
     ]
    }
   ],
   "source": [
    "print(beginning[\"kha\"])\n",
    "print(beginning[\"sh\"])\n",
    "print(beginning[\"j\"])\n",
    "print(beginning[\"ch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 45\n"
     ]
    }
   ],
   "source": [
    "with open('../Rules/middle.txt', 'r') as mid_file:\n",
    "    middle = {}\n",
    "    for count, line in enumerate(mid_file, start=1):\n",
    "        splited = line.strip().split(\" \")\n",
    "        key = splited[0]\n",
    "        values = splited[1:]\n",
    "        middle[key] = values\n",
    "\n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ش']\n",
      "['ی', 'یع', 'ئی', 'عی']\n",
      "['ک']\n",
      "['nothing', 'ع', 'ا', 'اع', 'أ', 'ؤ', 'ؤا', 'ئا', 'ئ']\n"
     ]
    }
   ],
   "source": [
    "print(middle[\"sh\"])\n",
    "print(middle[\"i\"])\n",
    "print(middle[\"ck\"])\n",
    "print(middle[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 48\n"
     ]
    }
   ],
   "source": [
    "with open('../Rules/ending.txt', 'r') as end_file:\n",
    "    ending = {}\n",
    "    for count, line in enumerate(end_file, start=1):\n",
    "        splited = line.strip().split(\" \")\n",
    "        key = splited[0]\n",
    "        values = splited[1:]\n",
    "        ending[key] = values\n",
    "\n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ش']\n",
      "['ی', 'یع', 'یی']\n",
      "['ک']\n",
      "['ا', 'اع', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "print(ending[\"sh\"])\n",
    "print(ending[\"i\"])\n",
    "print(ending[\"ck\"])\n",
    "print(ending[\"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 62\n"
     ]
    }
   ],
   "source": [
    "with open('../Rules/chars.txt', 'r') as abbreviation_words:\n",
    "    abbr = {}\n",
    "    for count, line in enumerate(abbreviation_words, start=1):\n",
    "        splited = line.strip().split(\" \")\n",
    "        key = splited[0]\n",
    "        values = splited[1:]\n",
    "        abbr[key] = values\n",
    "\n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.WordFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines 137255\n"
     ]
    }
   ],
   "source": [
    "with open('../Frequent Persian Words/persian_frequent_words.txt', 'r') as freq_file:\n",
    "    word_freq = {}\n",
    "    min_freq = 3\n",
    "    count = 0\n",
    "    for count, line in enumerate(freq_file, start=1):\n",
    "        word, freq = line.strip().split(\"\\t\")\n",
    "        freq = int(freq)\n",
    "        if freq < min_freq:\n",
    "            break\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = freq\n",
    "\n",
    "print(f\"number of lines {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84193\n",
      "24228\n"
     ]
    }
   ],
   "source": [
    "print(word_freq[\"یک\"])\n",
    "print(word_freq[\"جمعیت\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "\n",
    "def f2p_word_internal(word, original_word):\n",
    "    persian = []\n",
    "    for i, letter in enumerate(word):\n",
    "        if i == 0:\n",
    "            converter = beginning\n",
    "        elif i == len(word) - 1:\n",
    "            converter = ending\n",
    "        else:\n",
    "            converter = middle\n",
    "        conversions = converter.get(letter)\n",
    "        if conversions is None:\n",
    "            return [(original_word, 0.0)]\n",
    "        conversions = ['' if i == 'nothing' else i for i in conversions]\n",
    "        persian.append(conversions)\n",
    "\n",
    "    alternatives = itertools.product(*persian)\n",
    "    alternatives = [''.join(i) for i in alternatives]\n",
    "    \n",
    "    unicode_points = [1607, 8204]\n",
    "    reconstructed_string = ''.join(chr(point) for point in unicode_points)\n",
    "    for i in range(len(alternatives)):\n",
    "        if '@' in alternatives[i]:\n",
    "            alternatives[i] = alternatives[i].replace('@', reconstructed_string)\n",
    "\n",
    "    alternatives = [(i, word_freq[i]) if i in word_freq else (i, 0) for i in alternatives]\n",
    "    \n",
    "    if len(alternatives) > 0:\n",
    "        return [max(alternatives, key=lambda x: x[1])]\n",
    "    else:\n",
    "        return [(word, 1.0)]\n",
    "\n",
    "def f2p_word_internal_abbr(original_word, abbr):\n",
    "    persian = []\n",
    "\n",
    "    abbr_deepcopy = copy.deepcopy(abbr)\n",
    "    \n",
    "    for letter in original_word:\n",
    "        conversions = abbr_deepcopy.get(letter)\n",
    "        if conversions is None:\n",
    "            return [(original_word, 0.0)]\n",
    "        \n",
    "        conversion_plus_half_space = [conversion + chr(8204) for conversion in conversions]\n",
    "        conversions += conversion_plus_half_space\n",
    "        conversions = ['' if i == 'nothing' else i for i in conversions]\n",
    "        persian.append(conversions)\n",
    "\n",
    "    alternatives = itertools.product(*persian)\n",
    "    alternatives = [''.join(i) for i in alternatives]\n",
    "    alternatives = [(i, word_freq[i]) if i in word_freq else (i, 0) for i in alternatives]\n",
    "    \n",
    "    if len(alternatives) > 0:\n",
    "        return [max(alternatives, key=lambda x: x[1])]\n",
    "    else:\n",
    "        return [(original_word, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variations(word):\n",
    "    \"\"\"Create variations of the word based on letter combinations like oo, sh, etc.\"\"\"\n",
    "\n",
    "    if word == 'a':\n",
    "        return [['A']]\n",
    "    elif len(word) == 1:\n",
    "        return [[word[0]]]\n",
    "    elif word == 'aa':\n",
    "        return [['A']]\n",
    "    elif word == 'ee':\n",
    "        return [['i']]\n",
    "    elif word == 'iy':\n",
    "        return [['i']]\n",
    "    elif word == 'ei':\n",
    "        return [['ei']]\n",
    "    elif word in ['oo', 'ou']:\n",
    "        return [['u']]\n",
    "    elif word == 'kha':\n",
    "        return [['kha'], ['kh', 'a']]\n",
    "    elif word in ['kh', 'gh', 'ch', 'sh', 'zh', 'ck']:\n",
    "        return [[word]]\n",
    "    elif word in [\"'ee\", \"'ei\"]:\n",
    "        return [[\"'i\"]]\n",
    "    elif word in [\"'oo\", \"'ou\"]:\n",
    "        return [[\"'u\"]]\n",
    "    elif word in [\"a'\", \"e'\", \"o'\", \"i'\", \"u'\", \"A'\"]:\n",
    "        return [[word[0] + \"'\"]]\n",
    "    elif word in [\"'a\", \"'e\", \"'o\", \"'i\", \"'u\", \"'A\"]:\n",
    "        return [[\"'\" + word[1]]]\n",
    "    elif len(word) == 2 and word[0] == word[1]:\n",
    "        return [[word[0]]]\n",
    "\n",
    "    if word[:2] == 'aa':\n",
    "        return [['A'] + i for i in variations(word[2:])] + \\\n",
    "               [['a', 'a'] + i for i in variations(word[2:])]\n",
    "    elif word[:2] in ['ee', 'iy']:\n",
    "        return [['i'] + i for i in variations(word[2:])]\n",
    "    elif word[:2] in ['oo', 'ou']:\n",
    "        return [['u'] + i for i in variations(word[2:])]\n",
    "    elif word[:3] == 'kha':\n",
    "        return \\\n",
    "            [['kha'] + i for i in variations(word[3:])] + \\\n",
    "            [['kh', 'a'] + i for i in variations(word[3:])] + \\\n",
    "            [['k', 'h', 'a'] + i for i in variations(word[3:])]\n",
    "    elif word[:2] in ['kh', 'gh', 'ch', 'sh', 'zh', 'ck']:\n",
    "        return \\\n",
    "            [[word[:2]] + i for i in variations(word[2:])] \n",
    "           # [[word[0]] + i for i in variations(word[1:])]\n",
    "    elif word[:2] in [\"a'\", \"e'\", \"o'\", \"i'\", \"u'\", \"A'\"]:\n",
    "        return [[word[:2]] + i for i in variations(word[2:])]\n",
    "    elif word[:3] in [\"'ee\", \"'ei\"]:\n",
    "        return [[\"'i\"] + i for i in variations(word[3:])]\n",
    "    elif word[:3] in [\"'oo\", \"'ou\"]:\n",
    "        return [[\"'u\"] + i for i in variations(word[3:])]\n",
    "    elif word[:2] in [\"'a\", \"'e\", \"'o\", \"'i\", \"'u\", \"'A\"]:\n",
    "        return [[word[:2]] + i for i in variations(word[2:])]\n",
    "    elif len(word) >= 2 and word[0] == word[1]:\n",
    "        return [[word[0]] + i for i in variations(word[2:])]\n",
    "    else:\n",
    "        return [[word[0]] + i for i in variations(word[1:])]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p_word(word, max_word_size=30):\n",
    "    original_word = word\n",
    "    word = word.lower()  # Convert the word to lowercase\n",
    "\n",
    "    c = dictionary.get(word)\n",
    "    if c:\n",
    "        return c  # Return the word from the dictionary if it exists\n",
    "\n",
    "    if word == '':\n",
    "        return []\n",
    "    elif len(word) > max_word_size:\n",
    "        return [(original_word, 1.0)]  # Return the original word if it exceeds max_word_size\n",
    "\n",
    "    results = []\n",
    "    for w in variations(word):\n",
    "        results.extend(f2p_word_internal(w, original_word))  # Find variations of the word\n",
    "    \n",
    "    if len(original_word) <= 4:\n",
    "        results.extend(f2p_word_internal_abbr(original_word, abbr))  # Consider abbreviations if the word is short\n",
    "    \n",
    "    return max(results, key=lambda x:x[1])[0]  # Return the result with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_text(text):\n",
    "    # Define regex patterns for the three parts\n",
    "    first_part_pattern = r'^[^a-zA-Z0-9]*'\n",
    "    middle_part_pattern = r'[a-zA-Z0-9]+'\n",
    "    last_part_pattern = r'[^a-zA-Z0-9]*$'\n",
    "    \n",
    "    # Match the patterns against the text\n",
    "    first_part_match = re.match(first_part_pattern, text)\n",
    "    middle_part_match = re.search(middle_part_pattern, text)\n",
    "    last_part_match = re.search(last_part_pattern, text[middle_part_match.end():]) if middle_part_match else None\n",
    "    \n",
    "    # Extract the parts from the matches\n",
    "    first_part = first_part_match.group(0) if first_part_match else ''\n",
    "    middle_part = middle_part_match.group(0) if middle_part_match else ''\n",
    "    last_part = last_part_match.group(0) if last_part_match else text[middle_part_match.end():] if middle_part_match else text\n",
    "    \n",
    "    return [first_part, middle_part, last_part]\n",
    "\n",
    "def add_prefix_postfix(text, prefix, postfix):\n",
    "    \"\"\"\n",
    "    Adds a prefix and a postfix to the given text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The main text.\n",
    "        prefix (str): The prefix to add.\n",
    "        postfix (str): The postfix to add.\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with the prefix and postfix added.\n",
    "    \"\"\"\n",
    "    return prefix + text + postfix\n",
    "\n",
    "# Mapping of English punctuation and special characters to Persian equivalents\n",
    "english_to_persian = {\n",
    "    '!': '!',\n",
    "    '?': '؟',\n",
    "    '.': '.',\n",
    "    ',': '،',\n",
    "    ';': '؛',\n",
    "    ':': ':',\n",
    "    '\"': '\"',\n",
    "    \"'\": \"'\",\n",
    "    '(': '(',\n",
    "    ')': ')',\n",
    "    '[': '[',\n",
    "    ']': ']',\n",
    "    '{': '{',\n",
    "    '}': '}',\n",
    "    '/': '/',\n",
    "    '\\\\': '\\\\',\n",
    "    '@': '@',\n",
    "    '#': '#',\n",
    "    '$': '$',\n",
    "    '%': '%',\n",
    "    '^': '^',\n",
    "    '&': '&',\n",
    "    '*': '*',\n",
    "    '-': '-',\n",
    "    '_': '_',\n",
    "    '+': '+',\n",
    "    '=': '=',\n",
    "    '~': '~',\n",
    "    '`': '`',\n",
    "    '<': '<',\n",
    "    '>': '>'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2p(phrase, max_word_size=30):\n",
    "    \"\"\"\n",
    "    Converts an English phrase to its Persian equivalent, including handling punctuation and special characters.\n",
    "    \n",
    "    Parameters:\n",
    "        phrase (str): The English phrase to convert.\n",
    "        max_word_size (int): The maximum allowed size for a word to be converted.\n",
    "        \n",
    "    Returns:\n",
    "        str: The Persian equivalent of the phrase.\n",
    "    \"\"\"\n",
    "    # Split the phrase into words\n",
    "    splitted_phrase = re.split(r'[ -]', phrase)\n",
    "\n",
    "    word_parts = []\n",
    "    for word in splitted_phrase:\n",
    "        word_parts.append(split_text(word))  # Split each word into parts\n",
    "\n",
    "    # Convert special characters\n",
    "    for i in range(len(word_parts)):\n",
    "        for char in word_parts[i][0]:\n",
    "            word_parts[i][0] = word_parts[i][0].replace(char, english_to_persian[char])\n",
    "        for char in word_parts[i][2]:\n",
    "            word_parts[i][2] = word_parts[i][2].replace(char, english_to_persian[char]) \n",
    "\n",
    "    results = []\n",
    "    for word in word_parts:\n",
    "        results.append(f2p_word(word[1], max_word_size))  # Convert the middle part of each word to Persian\n",
    "\n",
    "    # Add prefix and postfix to the words\n",
    "    for i in range(len(results)):\n",
    "        if results[i] != []:\n",
    "            results[i] = add_prefix_postfix(results[i], word_parts[i][0], word_parts[i][2])\n",
    "        else:\n",
    "            if word_parts[i][0] != '':\n",
    "                results[i] = word_parts[i][0]\n",
    "            else:\n",
    "                results[i] = ' '\n",
    "\n",
    "    return ' '.join(results)  # Join the converted words into a single phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: سلام! دوست عزیز. چطوری؟\n",
      "Excepted output: سلام! دوست عزیز. چطوری؟\n"
     ]
    }
   ],
   "source": [
    "input = \"Salam! Doste aziz. Chetori?\"\n",
    "excepted_output = \"سلام! دوست عزیز. چطوری؟\"\n",
    "\n",
    "output = f2p(input)\n",
    "print(\"Output:\", output)\n",
    "print(\"Excepted output:\", excepted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: عکس\n",
      "Excepted output: عکس\n"
     ]
    }
   ],
   "source": [
    "input = \"ax\"\n",
    "excepted_output = \"عکس\"\n",
    "\n",
    "output = f2p(input)\n",
    "print(\"Output:\", output)\n",
    "print(\"Excepted output:\", excepted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: با ظهور مدلهای زبانی بزرگ میتوان در زمان کم خروجی مناسب دریافت کرد.\n",
      "Excepted output: با ظهور مدل‌های زبانی بزرگ می توان در زمان کم خروجی مناسب دریافت کرد.\n"
     ]
    }
   ],
   "source": [
    "input = \"Ba zohoore modelhaye zabani bozorg mitavan dar zamane kam khorooji monaseb daryaft kard.\"\n",
    "excepted_output = \"با ظهور مدل\\u200cهای زبانی بزرگ می توان در زمان کم خروجی مناسب دریافت کرد.\"\n",
    "\n",
    "output = f2p(input)\n",
    "print(\"Output:\", output)\n",
    "print(\"Excepted output:\", excepted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: میشه اینو فارسی کنی؟\n",
      "Excepted output: میشه اینو فارسی کنی؟\n"
     ]
    }
   ],
   "source": [
    "input = \"mishe ino farsi koni?\"\n",
    "excepted_output = \"میشه اینو فارسی کنی؟\"\n",
    "\n",
    "output = f2p(input)\n",
    "print(\"Output:\", output)\n",
    "print(\"Excepted output:\", excepted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: خیلی ممنون\n",
      "Excepted output: خیلی ممنون\n"
     ]
    }
   ],
   "source": [
    "input = \"kheili mamnoon\"\n",
    "excepted_output = \"خیلی ممنون\"\n",
    "\n",
    "output = f2p(input)\n",
    "print(\"Output:\", output)\n",
    "print(\"Excepted output:\", excepted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: امروز یه آرپی‌جی زدم به دشمن.!\n",
      "Excepted output: امروز یه آرپیجی زدم به دشمن.!\n"
     ]
    }
   ],
   "source": [
    "text = \"Emrooz ye rpg zadam be doshman.!\"\n",
    "excepted_output = \"امروز یه آرپیجی زدم به دشمن.!\"\n",
    "\n",
    "output = f2p(text)\n",
    "print(\"Output:\", output)\n",
    "print(\"Excepted output:\", excepted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingilish_sentences = [\n",
    "    \"Emrooz hava besyar delpazir bood.\",\n",
    "    \"Madaram ghaza-ye khoshmaze-i baraye sham dorost kardeh ast.\",\n",
    "    \"Farda be kuhpeymayi khahim raft.\",\n",
    "    \"Ketabe jadidi ke kharideh-am besyar jaleb ast.\",\n",
    "    \"Doost daram vaghti hava abri ast be park beravam.\",\n",
    "    \"Khaneman ra taze rang kardeh-im.\",\n",
    "    \"Har rooz sobh yek fenjan chai minusham.\",\n",
    "    \"Gorbehaman asheghe bazi ba kamva ast.\",\n",
    "    \"Pedaram hamishe mara be varzesh tashvigh mikonad.\",\n",
    "    \"Hafte ayande be didare doostanam miravam.\",\n",
    "    \"Yadgiriye zabane jadid mitavanad tajrobe jazabi bashad.\",\n",
    "    \"Emrooz ba khanevade be sinema raftim.\",\n",
    "    \"Man be golhaye hayateman rasidgi mikonam.\",\n",
    "    \"Har shab ghabl az khab kami motalee mikonam.\",\n",
    "    \"Emtehan hafte ayande kheyli mohem ast.\",\n",
    "    \"Az tamashaye ghorube aftab lezzat mibaram.\",\n",
    "    \"Behtarin zaman baraye piaderavi sobh zood ast.\",\n",
    "    \"Khaharam naghashihaye zibai mikeshad.\",\n",
    "    \"Karhaye honari ra kheyli doost daram.\",\n",
    "    \"Baraye rooze tavallode doostam yek hediye aali kharidam.\",\n",
    "    \"Man vaghti baran mibare kheyli delkhosh mishavam.\",\n",
    "    \"Har hafte ba doostam be kafe miravam.\",\n",
    "    \"Taaze yeksal ke piano zadan ro shoroo karde-am.\",\n",
    "    \"Khodraye nowi kharidim va kheyli razi hastim.\",\n",
    "    \"Har sobh be varzesh sabok mashghool mishavam.\",\n",
    "    \"Emshab gharar ast pizza dar khane dorost konim.\",\n",
    "    \"Az bazdid az mazare tarikhi lezzat mibaram.\",\n",
    "    \"In fasl, bagheman por az golhaye rangarang shodeh ast.\",\n",
    "    \"Mamoolan shabha filmhaye mostanad tamasha mikonam.\",\n",
    "    \"Dars haye khod ra be ezafeye honar va varzesh barname rizi mikonam.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_sentences = [\n",
    "    \"امروز هوا بسیار دلپذیر بود.\",\n",
    "    \"مادرم غذای خوشمزه‌ای برای شام درست کرده است.\",\n",
    "    \"فردا به کوهپیمایی خواهیم رفت.\",\n",
    "    \"کتاب جدیدی که خریده‌ام بسیار جالب است.\",\n",
    "    \"دوست دارم وقتی هوا ابری است به پارک بروم.\",\n",
    "    \"خانه‌مان را تازه رنگ کرده‌ایم.\",\n",
    "    \"هر روز صبح یک فنجان چای می‌نوشم.\",\n",
    "    \"گربه‌ام عاشق بازی با کاموا است.\",\n",
    "    \"پدرم همیشه مرا به ورزش تشویق می‌کند.\",\n",
    "    \"هفته آینده به دیدار دوستانم می‌روم.\",\n",
    "    \"یادگیری زبان جدید می‌تواند تجربه جذابی باشد.\",\n",
    "    \"امروز با خانواده به سینما رفتیم.\",\n",
    "    \"من به گل‌های حیاطمان رسیدگی می‌کنم.\",\n",
    "    \"هر شب قبل از خواب کمی مطالعه می‌کنم.\",\n",
    "    \"امتحان هفته آینده خیلی مهم است.\",\n",
    "    \"از تماشای غروب آفتاب لذت می‌برم.\",\n",
    "    \"بهترین زمان برای پیاده‌روی صبح زود است.\",\n",
    "    \"خواهرم نقاشی‌های زیبایی می‌کشد.\",\n",
    "    \"کارهای هنری را خیلی دوست دارم.\",\n",
    "    \"برای روز تولد دوستم یک هدیه عالی خریدم.\",\n",
    "    \"من وقتی باران می‌بارد خیلی دلخوش می‌شوم.\",\n",
    "    \"هر هفته با دوستم به کافه می‌روم.\",\n",
    "    \"تازه یک سال است که پیانو زدن را شروع کرده‌ام.\",\n",
    "    \"خودروی نوی خریدیم و خیلی راضی هستیم.\",\n",
    "    \"هر صبح به ورزش سبک مشغول می‌شوم.\",\n",
    "    \"امشب قرار است پیتزا در خانه درست کنیم.\",\n",
    "    \"از بازدید از مزارع تاریخی لذت می‌برم.\",\n",
    "    \"این فصل، باغمان پر از گل‌های رنگارنگ شده است.\",\n",
    "    \"معمولاً شب‌ها فیلم‌های مستند تماشا می‌کنم.\",\n",
    "    \"درس‌های خود را به اضافی هنر و ورزش برنامه‌ریزی می‌کنم.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "امروز هوا بسیار دلپذیر بود.\n",
      "امروز هوا بسیار دلپذیر بود.\n",
      "##############################\n",
      "مادرم غذا یه خوشمزه ای برای شام درست کرده است.\n",
      "مادرم غذای خوشمزه‌ای برای شام درست کرده است.\n",
      "##############################\n",
      "فرد به کوهپیمایی خواهیم رفت.\n",
      "فردا به کوهپیمایی خواهیم رفت.\n",
      "##############################\n",
      "کتاب جدیدی که خریده ام بسیار جالب است.\n",
      "کتاب جدیدی که خریده‌ام بسیار جالب است.\n",
      "##############################\n",
      "دوست دارم وقتی هوا ابری است به پارک بروم.\n",
      "دوست دارم وقتی هوا ابری است به پارک بروم.\n",
      "##############################\n",
      "خانمان را تازه رنگ کرده ایم.\n",
      "خانه‌مان را تازه رنگ کرده‌ایم.\n",
      "##############################\n",
      "هر روز صبح یک فنجان چای مینوشم.\n",
      "هر روز صبح یک فنجان چای می‌نوشم.\n",
      "##############################\n",
      "گربهمن عاشق بازی با کاموا است.\n",
      "گربه‌ام عاشق بازی با کاموا است.\n",
      "##############################\n",
      "پدرم همیشه مرا به ورزش تشویق میکند.\n",
      "پدرم همیشه مرا به ورزش تشویق می‌کند.\n",
      "##############################\n",
      "هفته آینده به دیدار دوستانم میروم.\n",
      "هفته آینده به دیدار دوستانم می‌روم.\n",
      "##############################\n",
      "یادگیری زبان جدید میتواند تجربه جذابی باشد.\n",
      "یادگیری زبان جدید می‌تواند تجربه جذابی باشد.\n",
      "##############################\n",
      "امروز با خانواده به سینما رفتیم.\n",
      "امروز با خانواده به سینما رفتیم.\n",
      "##############################\n",
      "من به گلهای حیاطمان رسیدگی میکنم.\n",
      "من به گل‌های حیاطمان رسیدگی می‌کنم.\n",
      "##############################\n",
      "هر شب قبل از خواب کمی مطالعه میکنم.\n",
      "هر شب قبل از خواب کمی مطالعه می‌کنم.\n",
      "##############################\n",
      "امتحان هفته آینده خیلی مهم است.\n",
      "امتحان هفته آینده خیلی مهم است.\n",
      "##############################\n",
      "از تماشای غروب آفتاب لذت میبرم.\n",
      "از تماشای غروب آفتاب لذت می‌برم.\n",
      "##############################\n",
      "بهترین زمان برای پیاده‌روی صبح زود است.\n",
      "بهترین زمان برای پیاده‌روی صبح زود است.\n",
      "##############################\n",
      "خواهرم نقاشیهای زیبایی میکشد.\n",
      "خواهرم نقاشی‌های زیبایی می‌کشد.\n",
      "##############################\n",
      "کارهای هنری را خیلی دوست دارم.\n",
      "کارهای هنری را خیلی دوست دارم.\n",
      "##############################\n",
      "برای روز تولد دوستم یک هدیه عالی خریدم.\n",
      "برای روز تولد دوستم یک هدیه عالی خریدم.\n",
      "##############################\n",
      "من وقتی باران میبرع خیلی دلخوش میشوم.\n",
      "من وقتی باران می‌بارد خیلی دلخوش می‌شوم.\n",
      "##############################\n",
      "هر هفته با دوستم به کف میروم.\n",
      "هر هفته با دوستم به کافه می‌روم.\n",
      "##############################\n",
      "تازه یکسال که پیانو زدن رو شروع کرد ام.\n",
      "تازه یک سال است که پیانو زدن را شروع کرده‌ام.\n",
      "##############################\n",
      "خدری نوی خاریدیم و خیلی رازی هستیم.\n",
      "خودروی نوی خریدیم و خیلی راضی هستیم.\n",
      "##############################\n",
      "هر صبح به ورزش سبک مشغول میشوم.\n",
      "هر صبح به ورزش سبک مشغول می‌شوم.\n",
      "##############################\n",
      "امشب قرار است پیتزا در خانه درست کنیم.\n",
      "امشب قرار است پیتزا در خانه درست کنیم.\n",
      "##############################\n",
      "از بازدید از مزار تاریخی لذت میبرم.\n",
      "از بازدید از مزارع تاریخی لذت می‌برم.\n",
      "##############################\n",
      "این فصل، بغمان پر از گلهای رنگارنگ شده است.\n",
      "این فصل، باغمان پر از گل‌های رنگارنگ شده است.\n",
      "##############################\n",
      "معمولان شبه فیلمهای مستند تماشا میکنم.\n",
      "معمولاً شب‌ها فیلم‌های مستند تماشا می‌کنم.\n",
      "##############################\n",
      "درس های خود را به اضافی هنر و ورزش برنامه ریزی میکنم.\n",
      "درس‌های خود را به اضافی هنر و ورزش برنامه‌ریزی می‌کنم.\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "for f_sentence, p_sentence in zip(fingilish_sentences, persian_sentences):\n",
    "    print(f2p(f_sentence))\n",
    "    print(p_sentence)\n",
    "    print(\"#\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
